#!/usr/bin/env python3
"""
Debug script to check embedding dimensions and identify the source of the 768 dimension.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def debug_embedding_dimension():
    """Debug the embedding dimension issue."""
    print("ğŸ” Debugging embedding dimension issue...")
    
    try:
        from src.models.embedder import embed, model
        
        print(f"ğŸ”§ Model: {model.model_name}")
        
        # Test the dynamic dimension detection
        print("\nğŸ“Š Testing dynamic dimension detection...")
        dimension = model.get_sentence_embedding_dimension()
        print(f"   Detected dimension: {dimension}")
        
        # Test with a simple text
        print("\nğŸ“ Testing actual embedding...")
        test_text = "Hello world"
        embedding = embed(test_text)
        actual_dimension = len(embedding)
        print(f"   Actual embedding dimension: {actual_dimension}")
        
        # Test with a longer text
        print("\nğŸ“ Testing with longer text...")
        long_text = "This is a longer text to test if the embedding dimension changes with different input lengths."
        long_embedding = embed(long_text)
        long_dimension = len(long_embedding)
        print(f"   Long text embedding dimension: {long_dimension}")
        
        # Check if dimensions are consistent
        print(f"\nğŸ” Dimension consistency check:")
        print(f"   Dynamic detection: {dimension}")
        print(f"   Short text: {actual_dimension}")
        print(f"   Long text: {long_dimension}")
        
        if dimension == actual_dimension == long_dimension:
            print(f"   âœ… All dimensions are consistent!")
        else:
            print(f"   âŒ Dimensions are inconsistent!")
            return False
        
        # Test batch embedding
        print(f"\nğŸ“ Testing batch embedding...")
        batch_texts = ["Text 1", "Text 2", "Text 3"]
        batch_embeddings = embed(batch_texts)
        batch_dimensions = [len(emb) for emb in batch_embeddings]
        print(f"   Batch embedding dimensions: {batch_dimensions}")
        
        if all(dim == dimension for dim in batch_dimensions):
            print(f"   âœ… Batch dimensions are consistent!")
        else:
            print(f"   âŒ Batch dimensions are inconsistent!")
            return False
        
        print(f"\nğŸ‰ All embedding tests passed!")
        print(f"ğŸ“Š Final dimension: {dimension}")
        return True
        
    except Exception as e:
        print(f"âŒ Error during debugging: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ” Debug Embedding Dimension Issue")
    print("=" * 50)
    
    success = debug_embedding_dimension()
    
    print("=" * 50)
    if success:
        print("âœ… Debug completed successfully!")
        print("ğŸ’¡ If you're still getting 768 dimensions, the issue might be:")
        print("   1. Cached embeddings from before the model change")
        print("   2. A different code path using an old embedding model")
        print("   3. Environment variable pointing to wrong model")
    else:
        print("âŒ Debug failed!")

#!/usr/bin/env python3
"""
PDF Summarization Module
Extracts text from PDFs and generates concise summaries using LLM
"""

import logging
import os
from typing import Dict, List, Any, Optional
from .pdf_ingestor import extract_text_pages
from ..models.LLM import LLM
from ..models.client_factory import get_llm_client, get_default_model

logger = logging.getLogger(__name__)

def summarize_pdf_content(
    pdf_path: str,
    max_pages: int = 50,
    summary_length: str = "medium",
    language: str = "zh-TW"
) -> Dict[str, Any]:
    """
    Extract text from PDF and generate a comprehensive summary using chunked approach.
    
    Args:
        pdf_path: Path to the PDF file
        max_pages: Maximum number of pages to process (to handle large PDFs)
        summary_length: "short", "medium", or "long" summary
        language: Language for the summary (default: Traditional Chinese)
        
    Returns:
        Dict containing:
        - summary: The generated summary text
        - page_count: Number of pages processed
        - total_chars: Total characters in original text
        - summary_chars: Characters in summary
        - chunks_processed: Number of chunks processed
    """
    try:
        logger.info(f"ğŸ“„ Starting PDF summarization for: {pdf_path}")
        
        # Extract text from PDF using existing function
        pages_data = extract_text_pages(pdf_path)
        total_pages = len(pages_data)
        
        if not pages_data:
            return {
                "summary": "",
                "page_count": 0,
                "total_chars": 0,
                "summary_chars": 0,
                "error": "No text content found in PDF"
            }
        
        # Limit pages for performance
        if total_pages > max_pages:
            logger.warning(f"âš ï¸ PDF has {total_pages} pages, limiting to {max_pages} for summarization")
            pages_data = pages_data[:max_pages]
        
        # Combine all text
        full_text = ""
        for page_data in pages_data:
            page_text = page_data.get("text", "").strip()
            if page_text:
                full_text += f"\n\n[ç¬¬{page_data['page']}é ]\n{page_text}"
        
        total_chars = len(full_text)
        logger.info(f"ğŸ“Š Extracted {total_chars} characters from {len(pages_data)} pages")
        
        # ğŸš¨ NEW: Character threshold validation
        # Based on timing data: 70 pages â‰ˆ 24 chunks â‰ˆ 3 minutes
        # Target: ~1-1.5 minutes max processing time
        # 24 chunks = ~48,000 chars (2000 chars per chunk)
        # For 1.5 minutes: ~30 chunks max = ~60,000 chars
        MAX_CHARS_THRESHOLD = 60000  # ~30 chunks = ~1.5 minutes processing time
        
        if total_chars > MAX_CHARS_THRESHOLD:
            estimated_chunks = total_chars // 1800  # 2000 - 200 overlap
            estimated_time_minutes = estimated_chunks * 0.075  # ~4.5 seconds per chunk
            
            error_msg = (
                f"PDF too large for chat processing. "
                f"Content: {total_chars:,} characters ({estimated_chunks} estimated chunks). "
                f"Estimated processing time: {estimated_time_minutes:.1f} minutes. "
                f"Please use the RAG ingestion pipeline for large documents."
            )
            
            logger.warning(f"ğŸš¨ {error_msg}")
            return {
                "summary": "",
                "page_count": len(pages_data),
                "total_chars": total_chars,
                "summary_chars": 0,
                "error": error_msg,
                "estimated_chunks": estimated_chunks,
                "estimated_time_minutes": estimated_time_minutes
            }
        
        if not full_text.strip():
            return {
                "summary": "",
                "page_count": len(pages_data),
                "total_chars": 0,
                "summary_chars": 0,
                "error": "No meaningful text content found"
            }
        
        # Check if we need chunked summarization (rough estimate: 1 token â‰ˆ 4 characters for Chinese)
        estimated_tokens = total_chars / 4
        logger.info(f"ğŸ“Š Content analysis: {total_chars} chars â‰ˆ {estimated_tokens:.0f} estimated tokens")
        
        if estimated_tokens > 6000:  # Leave buffer for prompts
            logger.info(f"ğŸ“Š Large content ({estimated_tokens:.0f} estimated tokens), using chunked summarization")
            try:
                summary = _generate_chunked_summary(full_text, summary_length, language)
                logger.info(f"âœ… Chunked summarization completed successfully")
            except Exception as chunk_error:
                logger.error(f"âŒ Chunked summarization failed: {chunk_error}")
                # Fallback to direct summarization with truncated text
                logger.info(f"ğŸ”„ Falling back to direct summarization with truncated text")
                truncated_text = full_text[:12000]  # ~3000 tokens
                summary = _generate_summary_with_llm(truncated_text, summary_length, language)
        else:
            logger.info(f"ğŸ“Š Small content ({estimated_tokens:.0f} estimated tokens), using direct summarization")
            summary = _generate_summary_with_llm(full_text, summary_length, language)
        
        result = {
            "summary": summary,
            "page_count": len(pages_data),
            "total_chars": total_chars,
            "summary_chars": len(summary),
            "compression_ratio": len(summary) / total_chars if total_chars > 0 else 0
        }
        
        logger.info(f"âœ… Generated summary: {len(summary)} chars (compression: {result['compression_ratio']:.2%})")
        return result
        
    except Exception as e:
        logger.error(f"âŒ PDF summarization failed: {e}")
        return {
            "summary": "",
            "page_count": 0,
            "total_chars": 0,
            "summary_chars": 0,
            "error": str(e)
        }


def _generate_summary_with_llm(
    text: str,
    summary_length: str = "medium",
    language: str = "zh-TW"
) -> str:
    """
    Generate summary using the existing LLM infrastructure.
    
    Args:
        text: Full text to summarize
        summary_length: "short", "medium", or "long"
        language: Target language for summary
        
    Returns:
        Generated summary text
    """
    try:
        # Define summary parameters based on length
        length_configs = {
            "short": {
                "target_words": "200-300å­—",
                "focus": "æœ€é‡è¦çš„é—œéµé»",
                "structure": "ç°¡æ½”æ‘˜è¦"
            },
            "medium": {
                "target_words": "500-800å­—", 
                "focus": "ä¸»è¦å…§å®¹å’Œé‡é»åˆ†æ",
                "structure": "çµæ§‹åŒ–æ‘˜è¦ï¼ŒåŒ…å«ä¸»è¦æ®µè½"
            },
            "long": {
                "target_words": "1000-1500å­—",
                "focus": "è©³ç´°å…§å®¹å’Œæ·±å…¥åˆ†æ",
                "structure": "å®Œæ•´æ‘˜è¦ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦ç´°ç¯€"
            }
        }
        
        config = length_configs.get(summary_length, length_configs["medium"])
        
        # Create system prompt for summarization
        system_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–‡ä»¶åˆ†æå¸«ï¼Œå°ˆé–€é€²è¡Œæ–‡ä»¶æ‘˜è¦ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ç”Ÿæˆä¸€å€‹é«˜è³ªé‡çš„æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æ‘˜è¦é•·åº¦ï¼š{config['target_words']}
2. èªè¨€ï¼šç¹é«”ä¸­æ–‡
3. é‡é»ï¼š{config['focus']}
4. çµæ§‹ï¼š{config['structure']}
5. ä¿æŒå®¢è§€ã€æº–ç¢ºï¼Œä¸æ·»åŠ æ–‡ä»¶ä¸­æ²’æœ‰çš„è³‡è¨Š
6. å¦‚æœæ˜¯è²¡å‹™æˆ–å•†æ¥­æ–‡ä»¶ï¼Œè«‹ç‰¹åˆ¥é—œæ³¨æ•¸å­—ã€è¶¨å‹¢ã€é‡è¦æ±ºç­–
7. ä½¿ç”¨æ¸…æ™°çš„æ®µè½çµæ§‹ï¼Œä¾¿æ–¼é–±è®€

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨é©ç•¶çš„æ®µè½åˆ†éš”
- é‡è¦æ•¸å­—å’Œæ—¥æœŸè¦ä¿ç•™
- é—œéµè¡“èªè¦æº–ç¢º
- é¿å…å†—é¤˜å’Œé‡è¤‡"""

        user_prompt = f"""è«‹ç‚ºä»¥ä¸‹æ–‡ä»¶å…§å®¹ç”Ÿæˆæ‘˜è¦ï¼š

{text}

è«‹ç”Ÿæˆä¸€å€‹{config['target_words']}çš„{config['structure']}ï¼š"""

        # Get LLM client and generate summary
        llm_client = get_llm_client()
        default_model = get_default_model()
        
        logger.info(f"ğŸ¤– Generating {summary_length} summary using LLM...")
        logger.info(f"ğŸ“Š Input text length: {len(text)} chars")
        logger.info(f"ğŸ“Š System prompt length: {len(system_prompt)} chars")
        logger.info(f"ğŸ“Š User prompt length: {len(user_prompt)} chars")
        
        # Use higher temperature for more natural summarization
        options = {
            "temperature": 0.3,
            "max_length": 2048 if summary_length == "long" else 1024
        }
        
        logger.info(f"ğŸ”§ LLM options: {options}")
        
        try:
            summary = LLM(
                llm_client,
                default_model,
                system_prompt,
                user_prompt,
                options=options,
                timeout=60.0,  # Longer timeout for summarization
                raw=True
            )
            
            logger.info(f"âœ… LLM call completed successfully: {len(summary)} chars returned")
            
        except Exception as llm_error:
            logger.error(f"âŒ LLM call failed: {llm_error}")
            logger.error(f"ğŸ“Š Failed with text length: {len(text)} chars")
            raise llm_error
        
        # Clean up the summary
        summary = summary.strip()
        if not summary:
            raise ValueError("LLM returned empty summary")
            
        logger.info(f"âœ… Summary generated successfully: {len(summary)} characters")
        return summary
        
    except Exception as e:
        logger.error(f"âŒ LLM summarization failed: {e}")
        # Return a basic fallback summary
        return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{str(e)}"


def _generate_chunked_summary(
    text: str,
    summary_length: str = "medium",
    language: str = "zh-TW"
) -> str:
    """
    Generate summary for large texts using two-stage summarization.
    Stage 1: Summarize small chunks with fast settings
    Stage 2: Summarize half-chunks, then combine
    
    Args:
        text: Full text to summarize
        summary_length: "short", "medium", or "long"
        language: Target language for summary
        
    Returns:
        Generated summary text
    """
    try:
        logger.info(f"ğŸ”„ Starting two-stage chunked summarization for {len(text)} characters")
        
        # Calculate safe chunk size (leaving room for prompts)
        max_chunk_chars = 2000
        overlap_chars = 200  # Overlap to maintain context between chunks
        
        # Split text into overlapping chunks
        chunks = []
        start = 0
        chunk_count = 0
        
        while start < len(text):
            end = start + max_chunk_chars
            if end > len(text):
                end = len(text)
            
            chunk = text[start:end]
            chunks.append(chunk)
            chunk_count += 1
            
            logger.info(f"ğŸ“„ Created chunk {chunk_count}: {start}-{end} chars ({len(chunk)} chars)")
            
            # Move start position by chunk size minus overlap
            start = start + max_chunk_chars - overlap_chars
            
            # Safety check to prevent infinite loop
            if start >= len(text) or start >= end:
                break
        
        logger.info(f"ğŸ“Š Split into {len(chunks)} chunks for processing")
        
        # Stage 1: Summarize each chunk with fast settings
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ Stage 1: Processing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)")
            
            try:
                # Use ultra-fast settings for chunk summarization
                chunk_summary = _generate_summary_with_llm_fast(chunk, language)
                
                if chunk_summary and chunk_summary.strip():
                    chunk_summaries.append(chunk_summary)
                    logger.info(f"âœ… Chunk {i+1} summarized: {len(chunk_summary)} chars")
                else:
                    logger.warning(f"âš ï¸ Chunk {i+1} produced empty summary")
                    
            except Exception as chunk_error:
                logger.error(f"âŒ Failed to summarize chunk {i+1}: {chunk_error}")
                continue
        
        if not chunk_summaries:
            raise ValueError("No valid summaries generated from chunks")
        
        # Stage 2: Split chunk summaries into two halves and summarize each
        logger.info(f"ğŸ“Š Stage 2: Processing {len(chunk_summaries)} chunk summaries")
        
        mid_point = len(chunk_summaries) // 2
        first_half = chunk_summaries[:mid_point]
        second_half = chunk_summaries[mid_point:]
        
        logger.info(f"ğŸ“Š Split into two halves: {len(first_half)} + {len(second_half)} summaries")
        
        # Summarize first half
        first_half_text = "\n\n".join(first_half)
        logger.info(f"ğŸ”„ Summarizing first half ({len(first_half_text)} chars)")
        first_summary = _generate_summary_with_llm_fast(first_half_text, language)
        
        # Summarize second half as continuation
        second_half_text = "\n\n".join(second_half)
        logger.info(f"ğŸ”„ Summarizing second half as continuation ({len(second_half_text)} chars)")
        second_summary = _generate_summary_with_llm_fast_continuation(second_half_text, first_summary, language)
        
        # Combine the two half-summaries
        combined_summary = f"{first_summary}\n\n{second_summary}"
        logger.info(f"ğŸ“Š Combined two half-summaries: {len(combined_summary)} chars")
        
        logger.info(f"âœ… Two-stage summarization complete: {len(combined_summary)} chars")
        return combined_summary
        
    except Exception as e:
        logger.error(f"âŒ Two-stage summarization failed: {e}")
        # Fallback: return a basic summary of the first chunk
        try:
            first_chunk = text[:3000]
            return _generate_summary_with_llm(first_chunk, "short", language)
        except:
            return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{str(e)}"


def _generate_summary_with_llm_fast(
    text: str,
    language: str = "zh-TW"
) -> str:
    """
    Generate ultra-fast summary for chunks using minimal settings.
    
    Args:
        text: Text to summarize
        language: Target language for summary
        
    Returns:
        Generated summary text
    """
    try:
        # Ultra-fast system prompt (minimal)
        system_prompt = """ä½ æ˜¯æ–‡ä»¶æ‘˜è¦å°ˆå®¶ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡ç”Ÿæˆç°¡æ½”æ‘˜è¦ï¼Œé‡é»çªå‡ºé—œéµä¿¡æ¯ã€‚"""
        
        user_prompt = f"""è«‹ç‚ºä»¥ä¸‹å…§å®¹ç”Ÿæˆç°¡æ½”æ‘˜è¦ï¼š

{text}

æ‘˜è¦ï¼š"""

        # Get LLM client
        llm_client = get_llm_client()
        default_model = get_default_model()
        
        logger.info(f"âš¡ Fast summarization: {len(text)} chars")
        
        # Ultra-fast settings
        options = {
            "temperature": 0.1,  # Very low for consistency
            "max_length": 512,   # Shorter for speed
            "top_p": 0.9,        # Faster generation
            "top_k": 40          # Faster generation
        }
        
        summary = LLM(
            llm_client,
            default_model,
            system_prompt,
            user_prompt,
            options=options,
            timeout=30.0,  # Shorter timeout
            raw=True
        )
        
        logger.info(f"âš¡ Fast summary completed: {len(summary)} chars")
        return summary.strip()
        
    except Exception as e:
        logger.error(f"âŒ Fast summarization failed: {e}")
        return f"æ‘˜è¦å¤±æ•—ï¼š{str(e)}"


def _generate_summary_with_llm_fast_continuation(
    text: str,
    previous_summary: str,
    language: str = "zh-TW"
) -> str:
    """
    Generate ultra-fast summary continuation that flows naturally from the previous summary.
    
    Args:
        text: Text to summarize
        previous_summary: The first half summary to continue from
        language: Target language for summary
        
    Returns:
        Generated continuation summary text
    """
    try:
        # Continuation-focused system prompt
        system_prompt = """ä½ æ˜¯æ–‡ä»¶æ‘˜è¦å°ˆå®¶ã€‚è«‹ç”Ÿæˆç°¡æ½”çš„æ‘˜è¦å»¶çºŒï¼Œè¦èˆ‡å‰æ–‡è‡ªç„¶éŠœæ¥ï¼Œé¿å…é‡è¤‡é–‹é ­èªå¥ã€‚"""
        
        user_prompt = f"""ä»¥ä¸‹æ˜¯ç¬¬ä¸€éƒ¨åˆ†çš„æ‘˜è¦ï¼š

{previous_summary}

è«‹ç‚ºä»¥ä¸‹å…§å®¹ç”Ÿæˆæ‘˜è¦å»¶çºŒï¼Œè¦èˆ‡ä¸Šæ–‡è‡ªç„¶éŠœæ¥ï¼š

{text}

æ‘˜è¦å»¶çºŒï¼š"""

        # Get LLM client
        llm_client = get_llm_client()
        default_model = get_default_model()
        
        logger.info(f"âš¡ Fast continuation summarization: {len(text)} chars")
        
        # Ultra-fast settings
        options = {
            "temperature": 0.1,  # Very low for consistency
            "max_length": 512,   # Shorter for speed
            "top_p": 0.9,        # Faster generation
            "top_k": 40          # Faster generation
        }
        
        summary = LLM(
            llm_client,
            default_model,
            system_prompt,
            user_prompt,
            options=options,
            timeout=30.0,  # Shorter timeout
            raw=True
        )
        
        logger.info(f"âš¡ Fast continuation completed: {len(summary)} chars")
        return summary.strip()
        
    except Exception as e:
        logger.error(f"âŒ Fast continuation summarization failed: {e}")
        return f"æ‘˜è¦å»¶çºŒå¤±æ•—ï¼š{str(e)}"


def extract_pdf_text_for_chat(pdf_path: str, max_chars: int = 50000) -> Dict[str, Any]:
    """
    Extract and prepare PDF text for immediate chat context (without summarization).
    Useful for smaller PDFs or when full text is needed.
    
    Args:
        pdf_path: Path to the PDF file
        max_chars: Maximum characters to extract (truncate if larger)
        
    Returns:
        Dict containing extracted text and metadata
    """
    try:
        logger.info(f"ğŸ“„ Extracting PDF text for chat: {pdf_path}")
        
        pages_data = extract_text_pages(pdf_path)
        
        if not pages_data:
            return {
                "text": "",
                "page_count": 0,
                "total_chars": 0,
                "truncated": False,
                "error": "No text content found"
            }
        
        # Build formatted text
        formatted_text = ""
        for page_data in pages_data:
            page_text = page_data.get("text", "").strip()
            if page_text:
                formatted_text += f"\n\n[ç¬¬{page_data['page']}é ]\n{page_text}"
        
        total_chars = len(formatted_text)
        truncated = False
        
        # Truncate if too long
        if total_chars > max_chars:
            formatted_text = formatted_text[:max_chars] + "\n\n[æ–‡ä»¶å…§å®¹å› é•·åº¦é™åˆ¶è€Œæˆªæ–·...]"
            truncated = True
            logger.warning(f"âš ï¸ PDF text truncated from {total_chars} to {max_chars} characters")
        
        return {
            "text": formatted_text.strip(),
            "page_count": len(pages_data),
            "total_chars": total_chars,
            "extracted_chars": len(formatted_text),
            "truncated": truncated
        }
        
    except Exception as e:
        logger.error(f"âŒ PDF text extraction failed: {e}")
        return {
            "text": "",
            "page_count": 0,
            "total_chars": 0,
            "truncated": False,
            "error": str(e)
        }

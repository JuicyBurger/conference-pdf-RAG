"""
Fine-tune Jina embedding model for Chinese financial documents
"""

import os
import json
import random
from typing import List, Tuple, Dict
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from torch.utils.data import DataLoader
import pandas as pd


class FinancialDocumentTrainer:
    """Fine-tune embedding model for Chinese financial documents"""
    
    def __init__(self, model_name: str = "jinaai/jina-embeddings-v2-base-zh"):
        self.model = SentenceTransformer(model_name)
        self.training_examples = []
        self.eval_examples = []
    
    def generate_training_data_from_pdfs(self, pdf_paths: List[str]) -> List[InputExample]:
        """
        Generate training pairs from existing PDF documents
        """
        from ..data.pdf_ingestor import build_page_nodes
        
        examples = []
        
        for pdf_path in pdf_paths:
            print(f"ğŸ“„ Processing {pdf_path}...")
            nodes = build_page_nodes(pdf_path)
            
            # Create positive pairs from same document/page
            doc_nodes = {}
            for node in nodes:
                key = f"{node['page']}"
                if key not in doc_nodes:
                    doc_nodes[key] = []
                doc_nodes[key].append(node['text'])
            
            # Generate positive pairs (same page content)
            for page_texts in doc_nodes.values():
                if len(page_texts) > 1:
                    for i in range(len(page_texts)):
                        for j in range(i + 1, len(page_texts)):
                            examples.append(InputExample(
                                texts=[page_texts[i], page_texts[j]], 
                                label=0.8  # High similarity for same page
                            ))
            
            # Generate negative pairs (different pages, different docs)
            all_texts = [node['text'] for node in nodes]
            for _ in range(min(100, len(all_texts) // 2)):  # Limit negative examples
                text1, text2 = random.sample(all_texts, 2)
                examples.append(InputExample(
                    texts=[text1, text2], 
                    label=0.2  # Low similarity for random pairs
                ))
        
        print(f"âœ… Generated {len(examples)} training examples")
        return examples
    
    def create_qa_training_data(self, qa_files: List[str]) -> List[InputExample]:
        """
        Create training data from Q&A pairs (JSON files)
        """
        examples = []
        
        for qa_file in qa_files:
            with open(qa_file, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            for item in qa_data:
                question = item.get('question', '')
                answer = item.get('answer', '')
                
                if question and answer:
                    # Q&A pairs should be highly similar
                    examples.append(InputExample(
                        texts=[question, answer],
                        label=0.9
                    ))
        
        print(f"âœ… Generated {len(examples)} Q&A training examples from JSON files")
        return examples
    
    def extract_qa_from_pdfs(self, qa_pdf_paths: List[str]) -> List[InputExample]:
        """
        Extract Q&A pairs from historical Q&A PDF documents
        """
        from ..data.pdf_ingestor import build_page_nodes
        import re
        
        examples = []
        
        for pdf_path in qa_pdf_paths:
            print(f"ğŸ“„ Extracting Q&A from {pdf_path}...")
            nodes = build_page_nodes(pdf_path)
            
            for node in nodes:
                text = node['text']
                
                # Extract Q&A patterns from text
                qa_pairs = self._parse_qa_patterns(text)
                
                for question, answer in qa_pairs:
                    examples.append(InputExample(
                        texts=[question, answer],
                        label=0.95  # High confidence for historical Q&A
                    ))
        
        print(f"âœ… Generated {len(examples)} Q&A training examples from PDF documents")
        return examples
    
    def _parse_qa_patterns(self, text: str) -> List[Tuple[str, str]]:
        """
        Parse Q&A patterns from text using Chinese patterns
        """
        qa_pairs = []
        
        # Common Chinese Q&A patterns
        patterns = [
            # Q: ... A: ...
            r'[Qqå•å•é¡Œ][:ï¼š]\s*([^Aç­”]*?)\s*[Aaç­”][:ï¼š]\s*([^Qå•]*?)(?=[QAå•ç­”]|$)',
            # å•é¡Œ: ... å›ç­”: ...
            r'å•é¡Œ[:ï¼š]\s*([^å›]*?)\s*å›ç­”[:ï¼š]\s*([^å•]*?)(?=å•é¡Œ|$)',
            # å•: ... ç­”: ...
            r'å•[:ï¼š]\s*([^ç­”]*?)\s*ç­”[:ï¼š]\s*([^å•]*?)(?=å•|$)',
            # Question followed by answer paragraph
            r'([^ã€‚]*?[ï¼Ÿ?][^ã€‚]*?)([ã€‚][^ï¼Ÿ?]*?(?=[^ã€‚]*?[ï¼Ÿ?]|$))',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
            for match in matches:
                if len(match) == 2:
                    question = match[0].strip()
                    answer = match[1].strip()
                    
                    # Validate Q&A quality
                    if (len(question) > 5 and len(answer) > 10 and 
                        question != answer):
                        qa_pairs.append((question, answer))
        
        return qa_pairs
    
    def create_financial_term_pairs(self) -> List[InputExample]:
        """
        Create training pairs for financial terminology
        """
        financial_pairs = [
            # Financial metrics (high similarity)
            ("ç‡Ÿæ¥­æ”¶å…¥", "ç‡Ÿæ”¶", 0.95),
            ("æ·¨åˆ©æ½¤", "ç¨…å¾Œæ·¨åˆ©", 0.9),
            ("æ¯›åˆ©ç‡", "æ¯›åˆ©", 0.85),
            ("è³‡ç”¢è² å‚µè¡¨", "è³‡ç”¢è² å‚µ", 0.9),
            ("ç¾é‡‘æµé‡", "ç¾é‡‘æµ", 0.95),
            ("è‚¡æ±æ¬Šç›Š", "è‚¡æ±å ±é…¬", 0.85),
            
            # Business terms (medium similarity)
            ("å¸‚å ´ä½”æœ‰ç‡", "å¸‚å ´ä»½é¡", 0.9),
            ("ç«¶çˆ­å„ªå‹¢", "æ ¸å¿ƒç«¶çˆ­åŠ›", 0.8),
            ("æ¥­å‹™æˆé•·", "ç‡Ÿæ¥­æˆé•·", 0.85),
            
            # Unrelated terms (low similarity)
            ("ç‡Ÿæ¥­æ”¶å…¥", "å“¡å·¥äººæ•¸", 0.1),
            ("è‚¡åƒ¹", "å¤©æ°£", 0.0),
            ("è²¡å‹™å ±è¡¨", "ç”¢å“ä»‹ç´¹", 0.2),
        ]
        
        examples = []
        for term1, term2, similarity in financial_pairs:
            examples.append(InputExample(
                texts=[term1, term2],
                label=similarity
            ))
        
        print(f"âœ… Generated {len(examples)} financial term examples")
        return examples
    
    def prepare_training_data(self, pdf_paths: List[str], qa_files: List[str] = None, qa_pdf_paths: List[str] = None):
        """Prepare all training data"""
        all_examples = []
        
        # Add PDF-based examples (document structure learning)
        all_examples.extend(self.generate_training_data_from_pdfs(pdf_paths))
        
        # Add Q&A examples from JSON files
        if qa_files:
            all_examples.extend(self.create_qa_training_data(qa_files))
        
        # Add Q&A examples from historical Q&A PDFs (VERY VALUABLE!)
        if qa_pdf_paths:
            all_examples.extend(self.extract_qa_from_pdfs(qa_pdf_paths))
        
        # Add financial term examples
        all_examples.extend(self.create_financial_term_pairs())
        
        # Split into train/eval
        random.shuffle(all_examples)
        split_idx = int(0.8 * len(all_examples))
        
        self.training_examples = all_examples[:split_idx]
        self.eval_examples = all_examples[split_idx:]
        
        print(f"ğŸ“Š Training data: {len(self.training_examples)} examples")
        print(f"ğŸ“Š Evaluation data: {len(self.eval_examples)} examples")
    
    def fine_tune(self, output_path: str = "models/chinese-financial-embeddings", epochs: int = 3):
        """Fine-tune the model"""
        
        if not self.training_examples:
            raise ValueError("No training data available. Call prepare_training_data() first.")
        
        # Create data loader
        train_dataloader = DataLoader(self.training_examples, shuffle=True, batch_size=16)
        
        # Define loss function
        train_loss = losses.CosineSimilarityLoss(self.model)
        
        # Create evaluator
        if self.eval_examples:
            sentences1 = [ex.texts[0] for ex in self.eval_examples]
            sentences2 = [ex.texts[1] for ex in self.eval_examples]
            scores = [ex.label for ex in self.eval_examples]
            
            evaluator = EmbeddingSimilarityEvaluator(
                sentences1, sentences2, scores,
                name="chinese-financial-eval"
            )
        else:
            evaluator = None
        
        # Fine-tune
        print(f"ğŸš€ Starting fine-tuning for {epochs} epochs...")
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            evaluator=evaluator,
            evaluation_steps=500,
            warmup_steps=100,
            output_path=output_path,
            save_best_model=True,
            show_progress_bar=True
        )
        
        print(f"âœ… Fine-tuning completed! Model saved to: {output_path}")
        return output_path


def main():
    """Main training script"""
    import glob
    
    # Get all document PDFs (regular business documents)
    pdf_paths = glob.glob("data/raw/*.pdf")
    
    # Get historical Q&A PDFs (separate folder recommended)
    qa_pdf_paths = glob.glob("data/qa_pdfs/*.pdf")  # Historical Q&A PDFs
    
    # Get Q&A JSON files
    qa_files = glob.glob("*.json")  # bulk_qa_zh.json, etc.
    
    if not pdf_paths and not qa_pdf_paths:
        print("âŒ No PDF files found in data/raw/ or data/qa_pdfs/")
        return
    
    print("ğŸ“‚ Found training data:")
    print(f"  - Document PDFs: {len(pdf_paths)}")
    print(f"  - Q&A PDFs: {len(qa_pdf_paths)}")
    print(f"  - Q&A JSON files: {len(qa_files)}")
    
    # Initialize trainer
    trainer = FinancialDocumentTrainer()
    
    # Prepare training data with historical Q&A PDFs
    trainer.prepare_training_data(
        pdf_paths=pdf_paths, 
        qa_files=qa_files,
        qa_pdf_paths=qa_pdf_paths  # Historical Q&A PDFs!
    )
    
    # Fine-tune
    model_path = trainer.fine_tune(epochs=3)
    
    print(f"ğŸ‰ Fine-tuned model ready at: {model_path}")
    print("ğŸ’¡ Update src/embedder.py to use the new model:")
    print(f'    model = SentenceTransformer("{model_path}")')
    print("\nğŸ”„ Then re-index all documents:")
    print('    python -m src.cli index "data/raw/*.pdf"')


if __name__ == "__main__":
    main() 
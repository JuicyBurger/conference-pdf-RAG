"""
QA pair generation functionality.

This module provides functions for generating QA pairs from documents.
"""

import re
import json
import uuid
import logging
import random
from typing import List, Dict, Any, Optional

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue

from src.config import QDRANT_URL, QDRANT_API_KEY, QDRANT_COLLECTION
from src.rag.retrieval.retrieval_service import retrieval_service
from src.models.reranker import rerank
from src.models.LLM import LLM
from src.models.client_factory import get_llm_client, get_default_model
from .json_utils import sanitize_json_library, sanitize_json_via_llm, extract_partial_qa
from ..utils import handle_errors, GenerationError, DatabaseError, setup_logger

# Configure logging
logger = setup_logger(__name__)

# Initialize clients using the factory
llm_client = get_llm_client()
default_model = get_default_model()
qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

@handle_errors(error_class=DatabaseError, fallback_return=[])
def fetch_doc_chunks(doc_id: str, limit: int = 1000):
    """
    Scroll Qdrant for all chunks where payload.doc_id == doc_id.
    Attach full payloads so you can see page/text.
    
    Args:
        doc_id: Document ID to fetch chunks for
        limit: Maximum number of chunks to fetch
        
    Returns:
        List of document chunks
    """
    # Build the filter condition
    scroll_filter = Filter(
        must=[FieldCondition(key="doc_id", match=MatchValue(value=doc_id))]
    )
    # Note: use scroll_filter, and ask for payload
    records, _ = qdrant.scroll(
        collection_name=QDRANT_COLLECTION,
        scroll_filter=scroll_filter,
        limit=limit,
        with_payload=True
    )
    return records

@handle_errors(error_class=GenerationError, fallback_return=[])
def generate_qa_pairs_for_doc(
    doc_id: str,
    num_pairs: int = 5,
    timeout: float = 120.0,
    context_top_k: int = 20
) -> List[Dict[str, Any]]:
    """
    Enhanced QA generation with multiple evidence sources and exact text snippets.
    1) Fetch all chunks for this doc_id
    2) Retrieve + rerank broader context using multiple seed queries
    3) Build rich context with detailed source tracking
    4) Generate QA with enhanced source information
    
    Args:
        doc_id: Document ID to generate QA pairs for
        num_pairs: Number of QA pairs to generate
        timeout: Timeout for LLM generation in seconds
        context_top_k: Maximum number of context chunks to use
        
    Returns:
        List of QA pairs
    """
    
    print(f"Number of pairs that will be generated: {num_pairs}")
    
    # 1) Fetch every chunk for this PDF
    records = fetch_doc_chunks(doc_id)
    if not records:
        return []

    # 2) Enhanced retrieval with multiple perspectives and more diverse sources
    
    # Use diverse queries to gather broader context
    query_templates = [
        "è²¡å‹™è¡¨ç¾èˆ‡ç²åˆ©èƒ½åŠ›åˆ†æ",
        "ç‡Ÿé‹ç­–ç•¥èˆ‡æ¥­å‹™ç™¼å±•", 
        "é¢¨éšªç®¡ç†èˆ‡å…¬å¸æ²»ç†",
        "å¸‚å ´å±•æœ›èˆ‡æˆé•·å‹•èƒ½",
        "å…¬å¸æ²»ç†èˆ‡å…§éƒ¨æ§åˆ¶",
        "ç”¢å“æŠ€è¡“èˆ‡ç ”ç™¼å‰µæ–°",
        "å¸‚å ´ç«¶çˆ­èˆ‡ç”¢æ¥­åœ°ä½",
        "è²¡å‹™é¢¨éšªèˆ‡è³‡æœ¬çµæ§‹"
    ]
    
    all_hits = []
    sources_meta = []
    seen_chunks = set()
    
    logger.info(f"ğŸ” Starting retrieval for {len(query_templates)} query templates")
    
    for i, query in enumerate(query_templates):
        logger.info(f"ğŸ“‹ Query {i+1}/{len(query_templates)}: {query}")
        
        # Get more results per query to increase diversity
        hits = retrieval_service.retrieve(query, top_k=context_top_k//3, score_threshold=0.2)  # Lower threshold, more results
        logger.info(f"   Retrieved {len(hits)} initial hits")
        
        # Rerank and keep more top results
        hits = rerank(query, hits)[:8]  # Top 8 per query (increased from 5)
        logger.info(f"   After reranking: {len(hits)} hits")
        
        for h in hits:
            if h.id not in seen_chunks:
                seen_chunks.add(h.id)
                all_hits.append(h)
                
                # Store detailed source metadata
                sources_meta.append({
                    "doc_id": doc_id,
                    "page": h.payload.get("page", "unknown"),
                    "text": h.payload.get("text", ""),
                    "chunk_id": h.id
                })
                logger.info(f"   âœ… Added chunk from page {h.payload.get('page', 'unknown')}")
    
    logger.info(f"ğŸ¯ Total unique chunks collected: {len(all_hits)}")
    
    # 3) Build context from diverse sources
    logger.info(f"ğŸ“š Building context from {len(all_hits)} hits (limiting to {context_top_k})")
    
    context_blocks = []
    for i, h in enumerate(all_hits[:context_top_k]):  # Limit total context
        page = h.payload.get("page", "unknown")
        text = h.payload.get("text", "")
        # Truncate for prompt efficiency but keep full text in metadata
        context_blocks.append(f"[ç¬¬{page}é ] {text}")
        logger.info(f"   ğŸ“„ Context block {i+1}: Page {page}, length {len(text)} chars")
    
    context = "\n\n".join(context_blocks)
    logger.info(f"ğŸ“ Final context length: {len(context)} characters")
    
    # 4) Enhanced system prompt for factual, specific QA with multiple sources
    system_prompt = f"""
    ä½ æ˜¯å°ˆæ¥­çš„æŠ•è³‡ç°¡å ±åŠ©æ‰‹ï¼Œæ“…é•·å¾å…¬å¸æ³•èªªæœƒç´€éŒ„èˆ‡å¹´å ±ä¸­æŒ–æ˜å…·å¸å¼•åŠ›çš„æŠ•è³‡äº®é»ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹ï¼Œç”Ÿæˆ**æ°å¥½ {num_pairs} çµ„**é«˜å“è³ªçš„å•ç­”å°ã€‚

    é‡è¦è¦æ±‚ï¼š
    1. **åš´æ ¼æ§åˆ¶æ•¸é‡**ï¼šå¿…é ˆç”Ÿæˆæ°å¥½ {num_pairs} çµ„å•ç­”å°ï¼Œä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘
    2. **ç­”æ¡ˆå¿…é ˆåŒ…å«å…·é«”äº‹å¯¦å’Œæ•¸æ“š**ï¼šæåˆ°å…·é«”æ•¸å­—ã€æ—¥æœŸã€äººåã€åˆ¶åº¦åç¨±ç­‰
    3. **è‡ªç„¶æµæš¢çš„èªè¨€**ï¼šä½¿ç”¨è‡ªç„¶çš„ç¹é«”ä¸­æ–‡ï¼Œé¿å…éæ–¼æ­£å¼çš„å…¬æ–‡ç”¨èª
    4. **è©³ç´°çš„å¯¦æ–½æ–¹å¼**ï¼šç•¶æåˆ°åˆ¶åº¦æˆ–æªæ–½æ™‚ï¼Œè¦èªªæ˜å…·é«”å¦‚ä½•å¯¦æ–½
    5. **ç­”æ¡ˆé•·åº¦ä¸é™**ï¼šå¯ä»¥å¯«å¾—è©³ç´°ä¸€äº›ï¼Œç¢ºä¿åŒ…å«æ‰€æœ‰é‡è¦äº‹å¯¦
    6. **å¤šå€‹ä¾†æºæ”¯æ´**ï¼šæ¯å€‹ç­”æ¡ˆå¿…é ˆåŸºæ–¼2-4å€‹ä¸åŒçš„ä¾†æºï¼Œç¢ºä¿ç­”æ¡ˆçš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
    7. **è©³ç´°çš„ä¾†æºè³‡è¨Š**ï¼šsource å¿…é ˆåŒ…å«å…·é«”çš„é ç¢¼å’Œæ–‡ä»¶è³‡è¨Š

    è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼š
    - å¿…é ˆæ˜¯æœ‰æ•ˆçš„ JSON é™£åˆ—
    - æ¯å€‹ç‰©ä»¶åŒ…å«ï¼šquestionã€answerã€source
    - source å¿…é ˆæ˜¯é™£åˆ—æ ¼å¼ï¼ŒåŒ…å«2-4å€‹ä¸åŒçš„ä¾†æºè³‡è¨Š
    - ç¢ºä¿æ‰€æœ‰å­—ä¸²éƒ½æœ‰æ­£ç¢ºçš„å¼•è™ŸåŒ…åœ
    - æœ€å¾Œä¸€å€‹ç‰©ä»¶å¾Œä¸èƒ½æœ‰é€—è™Ÿ

    ç¯„ä¾‹æ ¼å¼ï¼š
    [
    {{
        "question": "å…¬å¸è¿‘ä¸‰å¹´çš„ç‡Ÿæ”¶è¡¨ç¾å¦‚ä½•ï¼Ÿ",
        "answer": "æ ¹æ“šè²¡å‹™è³‡æ–™ï¼Œè¿‘ä¸‰å¹´ç‡Ÿæ”¶åˆ†åˆ¥ç‚ºï¼š2021å¹´1,521,567è¬å…ƒã€2022å¹´1,420,199è¬å…ƒã€2023å¹´1,325,971è¬å…ƒã€‚ç‡Ÿæ”¶å‘ˆç¾ç©©å¥æˆé•·è¶¨å‹¢ï¼Œä¸»è¦å—ç›Šæ–¼ç”¢å“çµæ§‹å„ªåŒ–åŠå¸‚å ´æ“´å±•ç­–ç•¥ã€‚",
        "source": [
            {{
                "doc_id": "112å¹´å ± 20240531",
                "page": 73,
                "text": "è¿‘ä¸‰å¹´ç‡Ÿæ”¶åˆ†åˆ¥ç‚ºï¼š2021å¹´1,521,567è¬å…ƒã€2022å¹´1,420,199è¬å…ƒã€2023å¹´1,325,971è¬å…ƒ..."
            }},
            {{
                "doc_id": "112å¹´å ± 20240531", 
                "page": 67,
                "text": "ç‡Ÿæ”¶åˆ†æé¡¯ç¤ºç©©å¥æˆé•·è¶¨å‹¢ï¼Œä¸»è¦å—ç›Šæ–¼ç”¢å“çµæ§‹å„ªåŒ–..."
            }},
            {{
                "doc_id": "112å¹´å ± 20240531",
                "page": 75,
                "text": "å¸‚å ´æ“´å±•ç­–ç•¥æœ‰æ•ˆæå‡ç‡Ÿæ”¶è¡¨ç¾..."
            }}
        ]
    }},
    {{
        "question": "å…¬å¸çš„å…§éƒ¨ç¨½æ ¸åˆ¶åº¦å¦‚ä½•é‹ä½œï¼Ÿ",
        "answer": "æˆ‘å€‘å»ºç«‹äº†å®Œæ•´çš„å…§éƒ¨ç¨½æ ¸åˆ¶åº¦ï¼ŒåŒ…æ‹¬ï¼šè¨‚æœ‰è©³ç´°çš„å…§éƒ¨ç¨½æ ¸åˆ¶åº¦åŠå„é …ç®¡ç†è¾¦æ³•ï¼Œè¦ç¯„å°å¤–å•†æ¥­æ´»å‹•ã€é‡‘éŒ¢å¾€ä¾†ã€åˆ©ç›Šè¡çªè¿´é¿åŠæ©Ÿå¯†è³‡æ–™ç®¡ç†ã€‚æ¯å¹´åŸ·è¡Œä¸€æ¬¡å…§éƒ¨è‡ªè©•ï¼Œè©•ä¼°æœŸé–“ç‚º112å¹´1æœˆ1æ—¥è‡³112å¹´12æœˆ31æ—¥ã€‚",
        "source": [
            {{
                "doc_id": "112å¹´å ± 20240531",
                "page": 49,
                "text": "å…§éƒ¨ç¨½æ ¸åˆ¶åº¦åŒ…æ‹¬è¨‚å®šèª ä¿¡ç¶“ç‡Ÿæ”¿ç­–åŠæ–¹æ¡ˆã€å»ºç«‹è‰¯å¥½å…¬å¸æ²»ç†åŠé¢¨éšªæ§ç®¡æ©Ÿåˆ¶..."
            }},
            {{
                "doc_id": "112å¹´å ± 20240531",
                "page": 21,
                "text": "å…§éƒ¨è‡ªè©•ï¼šæ¯å¹´åŸ·è¡Œä¸€æ¬¡ è©•ä¼°æœŸé–“ï¼š112 å¹´ 1 æœˆ 1 æ—¥è‡³ 112 å¹´ 12 æœˆ 31 æ—¥..."
            }}
        ]
    }}
    ]

    è«‹ç¢ºä¿è¼¸å‡ºçš„æ˜¯å®Œæ•´çš„ã€æœ‰æ•ˆçš„ JSON é™£åˆ—ï¼ŒåŒ…å«æ°å¥½ {num_pairs} å€‹ç‰©ä»¶ã€‚
    """
    user_prompt = f"""åŸºæ–¼ä»¥ä¸‹æ–‡ä»¶å…§å®¹ï¼Œç”Ÿæˆæ°å¥½ {num_pairs} çµ„é«˜å“è³ªçš„å•ç­”å°ã€‚

**é‡è¦æé†’**ï¼š
- å¿…é ˆç”Ÿæˆæ°å¥½ {num_pairs} çµ„å•ç­”å°
- ç­”æ¡ˆå¿…é ˆåŒ…å«æ–‡ä»¶ä¸­çš„å…·é«”æ•¸å­—ã€æ—¥æœŸã€äººåã€åˆ¶åº¦åç¨±ç­‰äº‹å¯¦
- **æ¯å€‹ç­”æ¡ˆå¿…é ˆåŸºæ–¼2-4å€‹ä¸åŒçš„ä¾†æº**ï¼Œç¢ºä¿ç­”æ¡ˆçš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
- ç¢ºä¿ JSON æ ¼å¼æ­£ç¢ºï¼Œæ‰€æœ‰å­—ä¸²éƒ½æœ‰å¼•è™ŸåŒ…åœ
- ä½¿ç”¨è‡ªç„¶çš„èªè¨€ï¼Œé¿å…éæ–¼æ­£å¼çš„å…¬æ–‡ç”¨èª
- ç­”æ¡ˆå¯ä»¥å¯«å¾—è©³ç´°ä¸€äº›ï¼Œæ•´åˆå¤šå€‹ä¾†æºçš„è³‡è¨Š

æ–‡ä»¶å…§å®¹ï¼š
{context}

è«‹è¼¸å‡ºå®Œæ•´çš„ JSON é™£åˆ—ï¼š"""

    # 5) Call LLM with enhanced settings for longer, more detailed answers
    logger.info(f"ğŸ¤– Starting LLM generation with {len(context_blocks)} context blocks")
    logger.info(f"ğŸ“Š Context summary: {len(context_blocks)} blocks, {len(context)} total chars")
    
    raw_response = LLM(
        llm_client, 
        default_model, 
        system_prompt, 
        user_prompt, 
        options={"temperature": 0.3, "max_length": 4096}, 
        timeout=timeout, 
        raw=True
    )
    
    logger.info(f"ğŸ“ Generated response length: {len(raw_response)} characters")
    print("LLM raw QA output:", raw_response)
    
    # 6) Parse JSON with robust sanitization
    logger.info("ğŸ” Parsing JSON response...")
    qa_list = []
    
    # Strategy 1: Try direct JSON parsing
    try:
        qa_list = json.loads(raw_response)
        logger.info(f"âœ… Successfully parsed {len(qa_list)} QA pairs")
    except json.JSONDecodeError as e:
        logger.warning(f"âŒ Direct JSON parsing failed: {e}")
        print(f"Raw response: {raw_response}")
        
        # Strategy 2: Try library-based sanitization
        try:
            cleaned_response = sanitize_json_library(raw_response)
            qa_list = json.loads(cleaned_response)
            logger.info("âœ… Fixed JSON parsing issues using library sanitization")
        except Exception as e:
            logger.warning(f"âŒ Library-based JSON fixing failed: {e}")
            
            # Strategy 3: Try LLM-based JSON repair (as last resort)
            try:
                logger.info("ğŸ”„ Attempting LLM-based JSON repair...")
                repaired_response = sanitize_json_via_llm(raw_response, llm_client, default_model, timeout=10)
                qa_list = json.loads(repaired_response)
                logger.info("âœ… Fixed JSON using LLM repair")
            except Exception as e2:
                logger.warning(f"âŒ LLM-based JSON repair failed: {e2}")
                
                # Strategy 4: Extract partial QA pairs
                try:
                    partial_qa = extract_partial_qa(raw_response)
                    if partial_qa:
                        qa_list = partial_qa
                        logger.info(f"âœ… Extracted {len(partial_qa)} partial QA pairs")
                    else:
                        raise ValueError("Could not extract any valid QA pairs")
                except Exception as e3:
                    logger.error(f"âŒ All JSON fixing methods failed: {e3}")
                    return []
    
    # Validate the parsed JSON structure
    if not isinstance(qa_list, list):
        logger.error("âŒ Parsed JSON is not a list")
        return []
    
    # Validate each QA pair
    valid_qa_list = []
    for i, item in enumerate(qa_list):
        if not isinstance(item, dict):
            logger.warning(f"âš ï¸ QA pair {i+1} is not a dictionary, skipping")
            continue
            
        # Check required fields
        if "question" not in item or "answer" not in item:
            logger.warning(f"âš ï¸ QA pair {i+1} missing required fields, skipping")
            continue
            
        # Ensure fields are strings
        if not isinstance(item["question"], str) or not isinstance(item["answer"], str):
            logger.warning(f"âš ï¸ QA pair {i+1} has non-string question/answer, skipping")
            continue
            
        # Add ID if missing
        if "id" not in item:
            item["id"] = str(uuid.uuid4())
            
        valid_qa_list.append(item)
    
    qa_list = valid_qa_list
    logger.info(f"âœ… Validated {len(qa_list)} QA pairs")
    
    # 7) Enhance each QA pair with detailed source information
    for item in qa_list:
        if "id" not in item:
            item["id"] = str(uuid.uuid4())
        
        # Transform simple source list to detailed source objects (if needed)
        old_source = item.get("source", [])
        
        # Check if source is already in detailed format
        if old_source and isinstance(old_source[0], dict):
            # Already in detailed format, skip transformation
            continue
            
        detailed_sources = []
        
        # Handle both string and list formats
        if isinstance(old_source, str):
            old_source = [old_source]
        
        # Map page references to actual source text
        for page_ref in old_source:
            if isinstance(page_ref, str):
                # Extract page number from reference like "ç¬¬18é "
                page_match = re.search(r'ç¬¬(\d+)é ', page_ref)
                if page_match:
                    page_num = int(page_match.group(1))
                    # Find matching sources
                    matching_sources = [s for s in sources_meta if s["page"] == page_num]
                    if matching_sources:
                        # Take first match, prepare detailed source
                        source = matching_sources[0]
                        detailed_source = {
                            "doc_id": source["doc_id"],
                            "page": source["page"],
                            "text": source["text"]
                        }
                        detailed_sources.append(detailed_source)
        
        # If no matches found, add some sources anyway (fallback)
        if not detailed_sources and sources_meta:
            # Use first few sources as fallback
            for source in sources_meta[:min(3, len(sources_meta))]:
                detailed_source = {
                    "doc_id": source["doc_id"],
                    "page": source["page"],
                    "text": source["text"]
                }
                detailed_sources.append(detailed_source)
        
        # Replace source field with enhanced data
        item["source"] = detailed_sources
    
    # 8) Quality checks and warnings
    logger.info("ğŸ” Quality checking QA pairs...")
    for i, item in enumerate(qa_list):
        num_sources = len(item.get("source", []))
        if num_sources < 2:
            logger.warning(f"âš ï¸ QA pair {i+1} has thin evidence: only {num_sources} source(s)")
        elif num_sources >= 3:
            logger.info(f"âœ… QA pair {i+1} has rich evidence: {num_sources} sources")
    
    # enforce exact count
    if len(qa_list) != num_pairs:
        logger.warning(f"âš ï¸ Expected {num_pairs} items but got {len(qa_list)}")
        # Truncate or pad to exact count
        if len(qa_list) > num_pairs:
            qa_list = qa_list[:num_pairs]
            logger.info(f"ğŸ“ Truncated to {num_pairs} items")
        else:
            logger.warning(f"âš ï¸ Cannot generate enough QA pairs, returning {len(qa_list)} items")
    
    logger.info(f"ğŸ‰ Final result: {len(qa_list)} QA pairs generated")
    return qa_list

# src/rag/suggestions/generator.py

import logging
from typing import List

from .catalog import QuestionSuggestion, store_questions, init_suggestion_collection
from ..generator import generate_qa_pairs_for_doc

logger = logging.getLogger(__name__)

def generate_questions_only(
    doc_id: str, 
    num_questions: int = 8,
    timeout: float = 30.0,
    chat_context: str = None
) -> List[str]:
    """
    Generate only questions (without answers) for faster suggestion generation.
    This is much lighter than full QA generation.
    """
    try:
        logger.info(f"ðŸ¤– Generating {num_questions} questions only for doc_id: {doc_id}")
        
        # Determine prompt based on whether we have chat context
        if chat_context:
            system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„AIåŠ©æ‰‹ã€‚åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹å’Œå°è©±æ­·å²ï¼Œç”Ÿæˆ {num_questions} å€‹ç°¡æ½”ã€æ·±åº¦ä¸”æœ‰åƒ¹å€¼çš„å•é¡Œã€‚

è¦æ±‚ï¼š
1. å•é¡Œè¦çµåˆæ–‡æª”å…§å®¹å’Œå°è©±è„ˆçµ¡
2. å•é¡Œè¦ç°¡æ½”æ˜Žçž­ï¼ŒæŽ§åˆ¶åœ¨ 30 å­—ä»¥å…§
3. å•é¡Œè¦æœ‰æ·±åº¦ï¼Œèƒ½å¼•ç™¼æ·±å…¥æ€è€ƒå’Œé€²ä¸€æ­¥æŽ¢è¨Ž
4. å•é¡Œè¦è‡ªç„¶åœ°å»¶çºŒå°è©±ä¸»é¡Œ
5. æ¯å€‹å•é¡Œéƒ½è¦å…·é«”ä¸”å¯å›žç­”
6. é¿å…é‡è¤‡å·²ç¶“è¨Žè«–éŽçš„å•é¡Œ
7. åªè¿”å›žå•é¡Œåˆ—è¡¨ï¼Œä¸è¦åŒ…å«ç­”æ¡ˆæˆ–è§£é‡‹

æ ¼å¼ï¼šç›´æŽ¥è¿”å›žå•é¡Œåˆ—è¡¨ï¼Œæ¯è¡Œä¸€å€‹å•é¡Œã€‚

è‰¯å¥½ç¯„ä¾‹ï¼š
- é€™å€‹è¶¨å‹¢èƒŒå¾Œçš„åŽŸå› æ˜¯ä»€éº¼ï¼Ÿ
- æœ‰å“ªäº›æ›¿ä»£æ–¹æ¡ˆå¯ä»¥è€ƒæ…®ï¼Ÿ
- é¢¨éšªè©•ä¼°çµæžœå¦‚ä½•ï¼Ÿ
- é æœŸçš„æ•ˆç›Šæœ‰å¤šå¤§ï¼Ÿ"""

            user_prompt = f"""åŸºæ–¼ä»¥ä¸‹æ–‡æª”å…§å®¹å’Œå°è©±æ­·å²ï¼Œç”Ÿæˆ {num_questions} å€‹ç°¡æ½”çš„å¾ŒçºŒå•é¡Œï¼ˆæ¯å€‹å•é¡ŒæŽ§åˆ¶åœ¨30å­—ä»¥å…§ï¼‰ï¼š

æ–‡æª”å…§å®¹ï¼š
{{doc_context}}

å°è©±æ­·å²ï¼š
{chat_context}

è«‹åŸºæ–¼æ–‡æª”å…§å®¹å’Œå°è©±è„ˆçµ¡ç”Ÿæˆ {num_questions} å€‹æœ‰æ„ç¾©çš„å•é¡Œï¼š"""
        else:
            system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡å‹™åˆ†æžå¸«ã€‚åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹ï¼Œç”Ÿæˆ {num_questions} å€‹ç°¡æ½”ã€æ·±åº¦ä¸”æœ‰åƒ¹å€¼çš„å•é¡Œã€‚

è¦æ±‚ï¼š
1. å•é¡Œå¿…é ˆåŸºæ–¼æ–‡æª”ä¸­çš„å…·é«”å…§å®¹
2. å•é¡Œè¦ç°¡æ½”æ˜Žçž­ï¼ŒæŽ§åˆ¶åœ¨ 30 å­—ä»¥å…§
3. å•é¡Œè¦æœ‰æ·±åº¦ï¼Œèƒ½å¼•ç™¼æ·±å…¥æ€è€ƒ
4. å•é¡Œè¦æ¶µè“‹ä¸åŒé¢å‘ï¼šè²¡å‹™ã€ç‡Ÿé‹ã€é¢¨éšªã€æ²»ç†ç­‰
5. æ¯å€‹å•é¡Œéƒ½è¦å…·é«”ä¸”å¯å›žç­”
6. é¿å…å†—é•·çš„èƒŒæ™¯æè¿°ï¼Œç›´æŽ¥æå‡ºæ ¸å¿ƒå•é¡Œ
7. åªè¿”å›žå•é¡Œåˆ—è¡¨ï¼Œä¸è¦åŒ…å«ç­”æ¡ˆæˆ–è§£é‡‹

æ ¼å¼ï¼šç›´æŽ¥è¿”å›žå•é¡Œåˆ—è¡¨ï¼Œæ¯è¡Œä¸€å€‹å•é¡Œã€‚

è‰¯å¥½ç¯„ä¾‹ï¼š
- ç‡Ÿæ”¶ä¸‹æ»‘çš„ä¸»è¦åŽŸå› æ˜¯ä»€éº¼ï¼Ÿ
- å…¬å¸å¦‚ä½•æ”¹å–„ç²åˆ©èƒ½åŠ›ï¼Ÿ
- ä¸»è¦ç«¶çˆ­é¢¨éšªæœ‰å“ªäº›ï¼Ÿ
- ç¾é‡‘æµé‡æ˜¯å¦å¥åº·ï¼Ÿ"""

            user_prompt = f"""åŸºæ–¼ä»¥ä¸‹æ–‡æª”å…§å®¹ï¼Œç”Ÿæˆ {num_questions} å€‹ç°¡æ½”æ·±åº¦å•é¡Œï¼ˆæ¯å€‹å•é¡ŒæŽ§åˆ¶åœ¨30å­—ä»¥å…§ï¼‰ï¼š

{{doc_context}}

è«‹ç”Ÿæˆ {num_questions} å€‹ç°¡æ½”å•é¡Œï¼š"""

        # Get document chunks for context
        from ..generator import fetch_doc_chunks
        records = fetch_doc_chunks(doc_id, limit=50)  # Reduced limit for speed
        
        if not records:
            logger.warning(f"No chunks found for doc_id: {doc_id}")
            return []
        
        # Build document context (just first 10 chunks)
        doc_context_blocks = []
        for record in records[:10]:  # Limit to first 10 chunks
            # Access Qdrant Record payload attributes
            page = record.payload.get("page", "æœªçŸ¥")
            text = record.payload.get("text", "")[:200]  # Limit text length
            doc_context_blocks.append(f"[ç¬¬{page}é ] {text}")
        
        doc_context = "\n\n".join(doc_context_blocks)
        
        # Call LLM with lighter parameters
        from ...models.LLM import LLM
        from ...models.client_factory import get_llm_client, get_default_model
        
        llm_client = get_llm_client()
        default_model = get_default_model()
        
        # Format the user prompt with appropriate context
        if chat_context:
            formatted_user_prompt = user_prompt.format(doc_context=doc_context)
        else:
            formatted_user_prompt = user_prompt.format(doc_context=doc_context)
        
        response = LLM(
            llm_client, 
            default_model, 
            system_prompt,
            formatted_user_prompt,
            options={"temperature": 0.3, "max_length": 1024},  # Much smaller for questions only
            timeout=timeout,
            raw=True
        )
        
        if not response:
            logger.warning("No response from LLM for question generation")
            return []
        
        # Parse questions from response
        questions = []
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('*'):
                # Clean up the question
                if line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')):
                    line = line[2:].strip()
                elif line.startswith(('-', 'â€¢', 'Â·')):
                    line = line[1:].strip()
                
                if line and len(line) > 10:  # Minimum question length
                    questions.append(line)
        
        # Limit to requested number
        questions = questions[:num_questions]
        
        logger.info(f"âœ… Generated {len(questions)} questions for doc_id: {doc_id}")
        return questions
        
    except Exception as e:
        logger.error(f"Failed to generate questions for doc_id {doc_id}: {e}")
        return []

def generate_suggestions_for_doc(
    doc_id: str, 
    num_questions: int = 10,
    auto_init_collection: bool = True,
    use_lightweight: bool = True,
    chat_context: str = None  # Optional chat context for room-based suggestions
) -> bool:
    """
    Generate question suggestions for a document and store them in the suggestion catalog.
    
    Args:
        doc_id: Document ID to generate suggestions for
        num_questions: Number of questions to generate
        auto_init_collection: Whether to auto-initialize the collection if it doesn't exist
        use_lightweight: Whether to use lightweight question-only generation (faster)
        chat_context: Optional chat conversation context for room-based suggestions
    
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        logger.info(f"ðŸ¤– Generating suggestions for doc_id: {doc_id}")
        
        # Initialize suggestion collection if needed
        if auto_init_collection:
            try:
                init_suggestion_collection()
            except Exception as e:
                # Collection might already exist, that's fine
                logger.debug(f"Collection init result: {e}")
        
        if use_lightweight:
            # Use lightweight question-only generation
            questions = generate_questions_only(
                doc_id=doc_id,
                num_questions=num_questions,
                timeout=30.0,  # Shorter timeout
                chat_context=chat_context
            )
            
            if not questions:
                logger.warning(f"No questions generated for doc_id: {doc_id}")
                return False
            
            # Convert questions to QuestionSuggestion objects
            suggestions = []
            for question_text in questions:
                # Create metadata based on whether this is room-based or document-based
                if chat_context:
                    sections = [{
                        "page": "chat_conversation",
                        "chunk_id": doc_id,
                        "source_text": "åŸºæ–¼æ–‡æª”å…§å®¹å’Œå°è©±æ­·å²ç”Ÿæˆ"
                    }]
                    tags = ["chat_room", "conversation", "contextual"]
                else:
                    sections = [{
                        "page": "æœªçŸ¥",
                        "chunk_id": doc_id,
                        "source_text": "åŸºæ–¼æ–‡æª”å…§å®¹ç”Ÿæˆ"
                    }]
                    tags = ["document"]
                
                suggestion = QuestionSuggestion(
                    question_text=question_text,
                    doc_id=doc_id,
                    sections=sections,
                    tags=tags,
                    popularity_score=0.0
                )
                
                suggestions.append(suggestion)
                
        else:
            # Use full QA generation (original method)
            qa_pairs = generate_qa_pairs_for_doc(
                doc_id=doc_id,
                num_pairs=num_questions,
                timeout=60.0
            )
            
            if not qa_pairs:
                logger.warning(f"No QA pairs generated for doc_id: {doc_id}")
                return False
            
            # Extract questions and convert to QuestionSuggestion objects
            suggestions = []
            for qa_pair in qa_pairs:
                try:
                    question_text = qa_pair.get("question", "").strip()
                    if not question_text:
                        continue
                    
                    # Extract section metadata from sources
                    sections = []
                    sources = qa_pair.get("sources", [])
                    
                    for source in sources:
                        section_info = {
                            "page": source.get("page"),
                            "chunk_id": source.get("doc_id"),
                            "source_text": source.get("text", "")[:200] + "..." if len(source.get("text", "")) > 200 else source.get("text", "")
                        }
                        sections.append(section_info)
                    
                    # Create suggestion object
                    suggestion = QuestionSuggestion(
                        question_text=question_text,
                        doc_id=doc_id,
                        sections=sections,
                        tags=None,
                        popularity_score=0.0
                    )
                    
                    suggestions.append(suggestion)
                    
                except Exception as e:
                    logger.warning(f"Failed to process QA pair: {e}")
                    continue
        
        if not suggestions:
            logger.warning(f"No valid suggestions extracted for doc_id: {doc_id}")
            return False
        
        # Store suggestions in catalog
        store_questions(suggestions)
        
        logger.info(f"âœ… Generated and stored {len(suggestions)} suggestions for doc_id: {doc_id}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to generate suggestions for doc_id {doc_id}: {e}")
        return False

def batch_generate_suggestions(
    doc_ids: List[str], 
    num_questions_per_doc: int = 8,
    use_lightweight: bool = True  # Default to lightweight
) -> dict:
    """
    Generate suggestions for multiple documents in batch.
    
    Args:
        doc_ids: List of document IDs
        num_questions_per_doc: Number of questions to generate per document
        use_lightweight: Whether to use lightweight generation (faster)
    
    Returns:
        dict: Results summary with success/failure counts
    """
    results = {
        "total": len(doc_ids),
        "successful": 0,
        "failed": 0,
        "failed_docs": []
    }
    
    logger.info(f"ðŸš€ Starting batch suggestion generation for {len(doc_ids)} documents (lightweight: {use_lightweight})")
    
    for i, doc_id in enumerate(doc_ids, 1):
        logger.info(f"Processing document {i}/{len(doc_ids)}: {doc_id}")
        
        success = generate_suggestions_for_doc(
            doc_id=doc_id,
            num_questions=num_questions_per_doc,
            auto_init_collection=(i == 1),
            use_lightweight=use_lightweight
        )
        
        if success:
            results["successful"] += 1
        else:
            results["failed"] += 1
            results["failed_docs"].append(doc_id)
    
    logger.info(f"ðŸŽ‰ Batch generation complete: {results['successful']} successful, {results['failed']} failed")
    return results
#!/usr/bin/env python3
"""
Retrieval Quality Testing Script

This script tests the retrieval system with a set of predefined questions
and evaluates the quality of responses.
"""

import json
import time
from datetime import datetime
from typing import List, Dict, Tuple, Any
import requests

# Test questions with difficulty levels
questions = [
    (1, "æ˜“", "113 å¹´åº¦çš„ç¸½ç¶“ç†æ˜¯èª°ï¼Ÿ"),
    (2, "æ˜“", "113 å¹´åº¦ç¸½ç¶“ç† A+B+C+D å››é …é…¬é‡‘ç¸½é¡ç‚ºå¤šå°‘ï¼Ÿä½”ç¨…å¾Œç´”ç›Šçš„æ¯”ä¾‹ç‚ºå¤šå°‘ï¼Ÿ"),
    (3, "æ˜“", "å…¬å¸ 113 å¹´åº¦å€‹é«”ç¨…å¾Œç´”ç›Šï¼ˆä»Ÿå…ƒï¼‰ç‚ºå¤šå°‘ï¼Ÿ"),
    (4, "æ˜“", "å‰¯ç¸½ç¶“ç†è·å‹™åœ¨ 113 å¹´åº¦æ˜¯å¦æœ‰äººä»»è·ï¼ˆæ˜¯å¦ç©ºç¼ºï¼‰ï¼Ÿ"),
    (5, "æ˜“", "113 å¹´åº¦è‘£äº‹æœƒå…±å¬é–‹å¹¾æ¬¡æœƒè­°ï¼Ÿ"),
    (6, "æ˜“", "å“ªä½ç¨ç«‹è‘£äº‹çš„å‡ºå¸­ç‡ç‚º 80%ï¼Ÿå…¶å¯¦éš›å‡ºå¸­èˆ‡å§”è¨—å‡ºå¸­æ¬¡æ•¸åˆ†åˆ¥ç‚ºå¤šå°‘ï¼Ÿ"),
    (7, "æ˜“", "è«‹åˆ—å‡ºè‘£äº‹æœƒå‡ºå¸­æƒ…å½¢è¡¨ä¸­çš„æ‰€æœ‰ç¨ç«‹è‘£äº‹å§“åã€‚"),
    (8, "ä¸­", "113 å¹´è‘£äº‹æœƒï¼å„å§”å“¡æœƒç¸¾æ•ˆè©•ä¼°çµæœæ˜¯æ–¼å“ªä¸€æ—¥æœŸå ±å‘Šè‡³å„æœƒè­°ï¼Ÿ"),
    (9, "ä¸­", "113 å¹´åº¦è©•é‘‘å¹³å‡ç¸½åˆ†ç‚ºä½•ï¼Ÿï¼ˆaï¼‰æ•´é«”è‘£äº‹æœƒï¼›ï¼ˆbï¼‰å€‹åˆ¥è‘£äº‹æˆå“¡ã€‚"),
    (10, "ä¸­", "ä¸‰å¤§åŠŸèƒ½æ€§å§”å“¡æœƒï¼ˆè–ªè³‡å ±é…¬ã€å¯©è¨ˆã€æåï¼‰çš„å¹³å‡ç¸½åˆ†å„ç‚ºå¤šå°‘ï¼Ÿ"),
    (11, "ä¸­", "åœ¨å…©å¹´åº¦æ¯”è¼ƒä¸­ï¼Œ113 å¹´è‘£äº‹é…¬é‡‘ç¸½é¡ç‚ºå¤šå°‘ï¼Ÿç›¸è¼ƒ 112 å¹´è®Šå‹•å¤šå°‘ï¼Ÿ"),
    (12, "ä¸­", "ï¼ˆå‰¯ï¼‰ç¸½ç¶“ç†é…¬é‡‘ç¸½é¡å æ¯”è‡ª 112â†’113 å¹´ä¸‹é™çš„ä¸»å› ç‚ºä½•ï¼Ÿ"),
    (13, "ä¸­", "æœ¬å…¬å¸è‘£äº‹é…¬é‡‘å ç¨…å¾Œç´”ç›Šä¹‹æ¯”ä¾‹åœ¨ 112 å¹´èˆ‡ 113 å¹´åˆ†åˆ¥ç‚ºä½•ï¼Ÿ"),
    (14, "ä¸­", "ï¼ˆå‰¯ï¼‰ç¸½ç¶“ç†é…¬é‡‘å ç¨…å¾Œç´”ç›Šä¹‹æ¯”ä¾‹åœ¨ 112 å¹´èˆ‡ 113 å¹´åˆ†åˆ¥ç‚ºä½•ï¼Ÿ"),
    (15, "ä¸­", "ä¾å…¬å¸æ”¿ç­–ï¼Œè‘£äº‹é…¬å‹å¾—è‡ªå¹´åº¦ç›ˆé¤˜æåˆ—ä¹‹ä¸Šé™æ¯”ä¾‹ç‚ºå¤šå°‘ï¼Ÿ"),
    (16, "ä¸­", "ä¾å…¬å¸æ”¿ç­–ï¼Œç•¶å¹´åº¦æœ‰ç›ˆé¤˜æ™‚ï¼Œå“¡å·¥é…¬å‹æ‡‰æåˆ—ä¹‹æ¯”ä¾‹ç‚ºå¤šå°‘ï¼Ÿ"),
    (17, "ä¸­", "113 å¹´åº¦åˆ†é…äºˆç¸½ç¶“ç†ä¹‹å“¡å·¥é…¬å‹ï¼ˆè‚¡ç¥¨ï¼ç¾é‡‘ï¼åˆè¨ˆï¼‰å„ç‚ºå¤šå°‘ï¼Ÿå…¶å æ¯”ç‚ºå¤šå°‘ï¼Ÿ"),
    (18, "é›£", "ä½•ç¨®æƒ…å½¢éœ€è¦æ­éœ²å€‹åˆ¥è‘£äº‹é…¬é‡‘è³‡è¨Šï¼Ÿè«‹åˆ—èˆ‰ä»»å…©é …æ¢ä»¶ã€‚"),
    (19, "é›£", "å› è¨­ç½®å¯©è¨ˆå§”å“¡æœƒè€Œä¸é©ç”¨è­‰åˆ¸äº¤æ˜“æ³•ç¬¬ 14 æ¢ä¹‹ 3 ä¹‹è¦å®šæ™‚ï¼Œå¯¦éš›é©ç”¨çš„æ˜¯å“ªä¸€æ¢ï¼Ÿ"),
    (20, "é›£", "æ ¹æ“šæä¾›çš„æ•¸æ“šï¼Œé©—è­‰ç¸½ç¶“ç† 3,640 ä»Ÿå…ƒç´„ç­‰æ–¼ 113 å¹´å€‹é«”ç¨…å¾Œç´”ç›Šçš„ 0.35%ã€‚è«‹å¯«å‡ºè¨ˆç®—å¼ï¼ˆåˆ†æ•¸ï¼‰ä¸¦çµ¦å‡ºçµæœã€‚"),
]

# API Configuration
API_BASE_URL = "http://127.0.0.1:5000"  # Adjust if your API runs on different port
CHAT_ENDPOINT = f"{API_BASE_URL}/api/chat/message"
STATUS_ENDPOINTS = [f"{API_BASE_URL}/health", f"{API_BASE_URL}/api/status/health"]

class RetrievalTester:
    def __init__(self):
        self.results = []
        self.start_time = None
        self.end_time = None
        self.total_api_time = 0.0  # Track total time spent on API calls
        
    def check_api_status(self) -> bool:
        """Check if the API is running (try multiple health endpoints)."""
        for url in STATUS_ENDPOINTS:
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    return True
            except Exception:
                continue
        return False
    
    def send_question(self, question: str, room_id: str | None = None) -> Dict[str, Any]:
        """Send a question to the chat API and get response (auto-create room when room_id=None)."""
        payload = {
            "content": question,
            "user_id": "test_script",
            "room_id": room_id,
            # chat route auto-creates room when None; no streaming in this test
        }
        
        try:
            api_start_time = time.time()  # Start timing API call
            response = requests.post(CHAT_ENDPOINT, json=payload, timeout=180)
            api_end_time = time.time()  # End timing API call
            api_call_time = api_end_time - api_start_time
            self.total_api_time += api_call_time  # Accumulate API time
            
            if response.status_code == 200:
                result = response.json()
                result['api_call_time'] = api_call_time  # Add timing info to result
                return result
            else:
                return {
                    "error": f"API returned status {response.status_code}",
                    "response": response.text,
                    "api_call_time": api_call_time
                }
        except Exception as e:
            return {"error": f"Request failed: {e}", "api_call_time": 0.0}
    
    def evaluate_response_quality(self, question: str, response: str, difficulty: str) -> Dict[str, Any]:
        """Evaluate the quality of a response based on various metrics."""
        evaluation = {
            "response_length": len(response),
            "has_error": "error" in response.lower() or "æŠ±æ­‰" in response or "ç„¡æ³•" in response,
            "has_numbers": any(char.isdigit() for char in response),
            "has_percentage": "%" in response,
            "has_chinese_numbers": any(char in "ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„" for char in response),
            "difficulty": difficulty
        }
        
        # Content relevance indicators
        relevance_keywords = {
            "ç¸½ç¶“ç†": ["ç¸½ç¶“ç†", "ç¶“ç†", "åŸ·è¡Œé•·", "CEO"],
            "é…¬é‡‘": ["é…¬é‡‘", "è–ªè³‡", "å ±é…¬", "è–ªè³‡å ±é…¬"],
            "è‘£äº‹": ["è‘£äº‹", "è‘£äº‹æœƒ", "ç¨ç«‹è‘£äº‹"],
            "ç¨…å¾Œç´”ç›Š": ["ç¨…å¾Œç´”ç›Š", "æ·¨åˆ©", "ç´”ç›Š", "ç¨…å¾Œ"],
            "æ¯”ä¾‹": ["æ¯”ä¾‹", "ç™¾åˆ†æ¯”", "%", "å æ¯”"],
            "å¹´åº¦": ["å¹´åº¦", "å¹´", "113å¹´", "112å¹´"],
            "æœƒè­°": ["æœƒè­°", "é–‹æœƒ", "å¬é–‹"],
            "å‡ºå¸­": ["å‡ºå¸­", "å‡ºå¸­ç‡", "å§”è¨—"],
            "å§”å“¡æœƒ": ["å§”å“¡æœƒ", "è–ªè³‡å ±é…¬å§”å“¡æœƒ", "å¯©è¨ˆå§”å“¡æœƒ", "æåå§”å“¡æœƒ"],
            "æ”¿ç­–": ["æ”¿ç­–", "è¦å®š", "æ³•è¦"],
            "å“¡å·¥é…¬å‹": ["å“¡å·¥é…¬å‹", "è‚¡ç¥¨", "ç¾é‡‘"],
            "æ­éœ²": ["æ­éœ²", "å…¬é–‹", "è³‡è¨Š"],
            "è­‰åˆ¸äº¤æ˜“æ³•": ["è­‰åˆ¸äº¤æ˜“æ³•", "ç¬¬14æ¢", "ç¬¬14æ¢ä¹‹3"]
        }
        
        # Check keyword relevance
        question_lower = question.lower()
        response_lower = response.lower()
        
        relevance_score = 0
        total_keywords = 0
        
        for category, keywords in relevance_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                total_keywords += 1
                if any(keyword in response_lower for keyword in keywords):
                    relevance_score += 1
        
        evaluation["keyword_relevance"] = relevance_score / max(total_keywords, 1)
        evaluation["total_keywords"] = total_keywords
        evaluation["matched_keywords"] = relevance_score
        
        # Overall quality score (0-10)
        quality_score = 0
        
        # Base score for having a response
        if not evaluation["has_error"]:
            quality_score += 3
        
        # Score for content length (not too short, not too long)
        if 50 <= evaluation["response_length"] <= 1000:
            quality_score += 2
        elif evaluation["response_length"] > 1000:
            quality_score += 1
        
        # Score for keyword relevance
        quality_score += evaluation["keyword_relevance"] * 3
        
        # Score for having numerical content (important for financial questions)
        if evaluation["has_numbers"] or evaluation["has_percentage"]:
            quality_score += 1
        
        # Bonus for Chinese numbers (common in financial documents)
        if evaluation["has_chinese_numbers"]:
            quality_score += 0.5
        
        evaluation["quality_score"] = min(quality_score, 10)
        
        return evaluation
    
    def run_test(self, question_id: int, difficulty: str, question: str) -> Dict[str, Any]:
        """Run a single test question."""
        print(f"\nğŸ” Testing Question {question_id} ({difficulty}): {question}")
        
        # Send question to API (auto-create room)
        response_data = self.send_question(question, room_id=None)
        
        # Extract API call time
        api_call_time = response_data.get('api_call_time', 0.0)
        
        if "error" in response_data:
            result = {
                "question_id": question_id,
                "difficulty": difficulty,
                "question": question,
                "response": f"Error: {response_data['error']}",
                "evaluation": {
                    "quality_score": 0,
                    "has_error": True,
                    "response_length": 0,
                    "keyword_relevance": 0
                },
                "api_call_time": api_call_time,
                "timestamp": datetime.now().isoformat()
            }
        else:
            # Extract response text from the actual API structure
            if response_data.get("status") == "success" and "data" in response_data:
                # Get the last AI message from the messages array
                messages = response_data["data"].get("messages", [])
                ai_messages = [msg for msg in messages if msg.get("role") == "ai"]
                
                if ai_messages:
                    # Get the most recent AI response
                    response_text = ai_messages[-1].get("content", "No response received")
                else:
                    response_text = "No AI response found"
            else:
                response_text = response_data.get("response", "No response received")
            
            # Evaluate response quality
            evaluation = self.evaluate_response_quality(question, response_text, difficulty)
            
            result = {
                "question_id": question_id,
                "difficulty": difficulty,
                "question": question,
                "response": response_text,
                "evaluation": evaluation,
                "api_call_time": api_call_time,
                "timestamp": datetime.now().isoformat()
            }
        
        # Print result summary
        score = result["evaluation"]["quality_score"]
        print(f"   ğŸ“Š Quality Score: {score:.1f}/10")
        print(f"   ğŸ“ Response Length: {result['evaluation']['response_length']} chars")
        print(f"   ğŸ¯ Keyword Relevance: {result['evaluation']['keyword_relevance']:.2f}")
        print(f"   â±ï¸  API Call Time: {api_call_time:.2f}s")
        
        return result
    
    def run_all_tests(self) -> List[Dict[str, Any]]:
        """Run all test questions."""
        print("ğŸš€ Starting Retrieval Quality Test")
        print("=" * 50)
        
        # Check API status
        if not self.check_api_status():
            print("âŒ API is not accessible. Please make sure the API is running.")
            return []
        
        print("âœ… API is accessible")
        
        self.start_time = time.time()
        
        for question_id, difficulty, question in questions:
            result = self.run_test(question_id, difficulty, question)
            self.results.append(result)
            
            # Small delay between requests
            time.sleep(1)
        
        self.end_time = time.time()
        
        return self.results
    
    def generate_summary(self) -> Dict[str, Any]:
        """Generate a comprehensive summary of test results."""
        if not self.results:
            return {"error": "No results to summarize"}
        
        total_questions = len(self.results)
        total_time = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        # Calculate overall statistics
        quality_scores = [r["evaluation"]["quality_score"] for r in self.results]
        avg_quality = sum(quality_scores) / len(quality_scores)
        
        # API call time statistics
        api_call_times = [r.get("api_call_time", 0.0) for r in self.results]
        total_api_time = sum(api_call_times)
        avg_api_call_time = total_api_time / len(api_call_times) if api_call_times else 0
        min_api_call_time = min(api_call_times) if api_call_times else 0
        max_api_call_time = max(api_call_times) if api_call_times else 0
        
        # Calculate overhead time (total time - API time)
        overhead_time = total_time - total_api_time
        overhead_percentage = (overhead_time / total_time * 100) if total_time > 0 else 0
        
        # Statistics by difficulty
        difficulty_stats = {}
        for difficulty in ["æ˜“", "ä¸­", "é›£"]:
            diff_results = [r for r in self.results if r["difficulty"] == difficulty]
            if diff_results:
                diff_scores = [r["evaluation"]["quality_score"] for r in diff_results]
                diff_api_times = [r.get("api_call_time", 0.0) for r in diff_results]
                difficulty_stats[difficulty] = {
                    "count": len(diff_results),
                    "avg_score": sum(diff_scores) / len(diff_scores),
                    "min_score": min(diff_scores),
                    "max_score": max(diff_scores),
                    "avg_api_call_time": sum(diff_api_times) / len(diff_api_times),
                    "total_api_time": sum(diff_api_times)
                }
        
        # Error analysis
        error_count = sum(1 for r in self.results if r["evaluation"]["has_error"])
        success_rate = (total_questions - error_count) / total_questions * 100
        
        # Response length analysis
        response_lengths = [r["evaluation"]["response_length"] for r in self.results]
        avg_length = sum(response_lengths) / len(response_lengths)
        
        # Keyword relevance analysis
        keyword_relevance = [r["evaluation"]["keyword_relevance"] for r in self.results]
        avg_relevance = sum(keyword_relevance) / len(keyword_relevance)
        
        summary = {
            "test_summary": {
                "total_questions": total_questions,
                "total_time_seconds": total_time,
                "avg_time_per_question": total_time / total_questions if total_questions > 0 else 0,
                "success_rate_percent": success_rate,
                "error_count": error_count
            },
            "api_timing": {
                "total_api_time_seconds": total_api_time,
                "avg_api_call_time_seconds": avg_api_call_time,
                "min_api_call_time_seconds": min_api_call_time,
                "max_api_call_time_seconds": max_api_call_time,
                "overhead_time_seconds": overhead_time,
                "overhead_percentage": overhead_percentage,
                "api_efficiency_percentage": (total_api_time / total_time * 100) if total_time > 0 else 0
            },
            "quality_metrics": {
                "overall_avg_score": avg_quality,
                "min_score": min(quality_scores),
                "max_score": max(quality_scores),
                "avg_response_length": avg_length,
                "avg_keyword_relevance": avg_relevance
            },
            "difficulty_breakdown": difficulty_stats,
            "top_performers": sorted(self.results, key=lambda x: x["evaluation"]["quality_score"], reverse=True)[:5],
            "bottom_performers": sorted(self.results, key=lambda x: x["evaluation"]["quality_score"])[:5],
            "fastest_responses": sorted(self.results, key=lambda x: x.get("api_call_time", 0.0))[:5],
            "slowest_responses": sorted(self.results, key=lambda x: x.get("api_call_time", 0.0), reverse=True)[:5]
        }
        
        return summary
    
    def print_summary(self, summary: Dict[str, Any]):
        """Print a formatted summary of test results."""
        print("\n" + "=" * 60)
        print("ğŸ“Š RETRIEVAL QUALITY TEST SUMMARY")
        print("=" * 60)
        
        # Test overview
        test_summary = summary["test_summary"]
        print(f"\nğŸ” Test Overview:")
        print(f"   â€¢ Total Questions: {test_summary['total_questions']}")
        print(f"   â€¢ Total Time: {test_summary['total_time_seconds']:.1f} seconds")
        print(f"   â€¢ Avg Time per Question: {test_summary['avg_time_per_question']:.1f} seconds")
        print(f"   â€¢ Success Rate: {test_summary['success_rate_percent']:.1f}%")
        print(f"   â€¢ Error Count: {test_summary['error_count']}")
        
        # API timing analysis
        api_timing = summary["api_timing"]
        print(f"\nâ±ï¸  API Timing Analysis:")
        print(f"   â€¢ Total API Time: {api_timing['total_api_time_seconds']:.1f} seconds")
        print(f"   â€¢ Avg API Call Time: {api_timing['avg_api_call_time_seconds']:.2f} seconds")
        print(f"   â€¢ API Call Range: {api_timing['min_api_call_time_seconds']:.2f}s - {api_timing['max_api_call_time_seconds']:.2f}s")
        print(f"   â€¢ Overhead Time: {api_timing['overhead_time_seconds']:.1f} seconds ({api_timing['overhead_percentage']:.1f}%)")
        print(f"   â€¢ API Efficiency: {api_timing['api_efficiency_percentage']:.1f}%")
        
        # Quality metrics
        quality_metrics = summary["quality_metrics"]
        print(f"\nğŸ“ˆ Quality Metrics:")
        print(f"   â€¢ Overall Avg Score: {quality_metrics['overall_avg_score']:.2f}/10")
        print(f"   â€¢ Score Range: {quality_metrics['min_score']:.1f} - {quality_metrics['max_score']:.1f}")
        print(f"   â€¢ Avg Response Length: {quality_metrics['avg_response_length']:.0f} characters")
        print(f"   â€¢ Avg Keyword Relevance: {quality_metrics['avg_keyword_relevance']:.2f}")
        
        # Difficulty breakdown
        print(f"\nğŸ“Š Performance by Difficulty:")
        for difficulty, stats in summary["difficulty_breakdown"].items():
            print(f"   â€¢ {difficulty} ({stats['count']} questions):")
            print(f"     - Avg Score: {stats['avg_score']:.2f}/10")
            print(f"     - Score Range: {stats['min_score']:.1f} - {stats['max_score']:.1f}")
            print(f"     - Avg API Time: {stats['avg_api_call_time']:.2f}s")
            print(f"     - Total API Time: {stats['total_api_time']:.1f}s")
        
        # Top performers
        print(f"\nğŸ† Top 5 Performers:")
        for i, result in enumerate(summary["top_performers"], 1):
            score = result["evaluation"]["quality_score"]
            api_time = result.get("api_call_time", 0.0)
            print(f"   {i}. Q{result['question_id']} ({result['difficulty']}): {score:.1f}/10 ({api_time:.2f}s)")
            print(f"      \"{result['question'][:50]}...\"")
        
        # Bottom performers
        print(f"\nâš ï¸ Bottom 5 Performers:")
        for i, result in enumerate(summary["bottom_performers"], 1):
            score = result["evaluation"]["quality_score"]
            api_time = result.get("api_call_time", 0.0)
            print(f"   {i}. Q{result['question_id']} ({result['difficulty']}): {score:.1f}/10 ({api_time:.2f}s)")
            print(f"      \"{result['question'][:50]}...\"")
        
        # Fastest responses
        print(f"\nâš¡ Fastest 5 Responses:")
        for i, result in enumerate(summary["fastest_responses"], 1):
            api_time = result.get("api_call_time", 0.0)
            score = result["evaluation"]["quality_score"]
            print(f"   {i}. Q{result['question_id']} ({result['difficulty']}): {api_time:.2f}s (Score: {score:.1f}/10)")
            print(f"      \"{result['question'][:50]}...\"")
        
        # Slowest responses
        print(f"\nğŸŒ Slowest 5 Responses:")
        for i, result in enumerate(summary["slowest_responses"], 1):
            api_time = result.get("api_call_time", 0.0)
            score = result["evaluation"]["quality_score"]
            print(f"   {i}. Q{result['question_id']} ({result['difficulty']}): {api_time:.2f}s (Score: {score:.1f}/10)")
            print(f"      \"{result['question'][:50]}...\"")
    
    def save_results(self, filename: str = None):
        """Save test results to a JSON file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"retrieval_test_results_{timestamp}.json"
        
        data = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "total_questions": len(self.results),
                "api_base_url": API_BASE_URL
            },
            "results": self.results,
            "summary": self.generate_summary()
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Results saved to: {filename}")

def main():
    """Main function to run the retrieval quality test."""
    tester = RetrievalTester()
    
    # Run all tests
    results = tester.run_all_tests()
    
    if results:
        # Generate and print summary
        summary = tester.generate_summary()
        tester.print_summary(summary)
        
        # Save results
        tester.save_results()
    else:
        print("âŒ No test results generated. Please check API connectivity.")

if __name__ == "__main__":
    main()
